{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework #7 AI Testbeds\n",
    "\n",
    "##### Sambanova\n",
    "https://github.com/omendibleba/ai-science-training-series/tree/main/07_AITestbeds/Sambanova\n",
    "\n",
    "Homework\n",
    "\n",
    "For BERT example, understand flags used in the script. Change values for flag --ntasks and measure its effect on performance.\n",
    "\n",
    "-n, --ntasks=ntasks    defined the   number of tasks to run\n",
    "\n",
    "We compiles and trianed the models using 8, 16 and 32 task, and the duration times were 784,805, and 787 seconds, respectively. NO significant trend is observed with this data on theeffect of the aount of task and the performance.\n",
    "\n",
    "Relvant files are availabel in the folders bert/ntask_{8,16,32}. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Graphcore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run MNIST example by changing values of the input parameters like batch-size, learning rate and number of epochs trained and observe and report the performance implications.\n",
    "\n",
    "The parameters for the original model were:\n",
    "\n",
    "learning rate = 0.03\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "batch_size = 8  \n",
    "\n",
    "test_batch_size = 80\n",
    "\n",
    "The only parameter changed for the Homework was the number of epochs which was increased to 20. With the riginal number of epochs the accuracy within the test set was of 98.06% which suggest the model is very accurate and might not necessarilly required a signifincantly larger number of steps.\n",
    "\n",
    "The increase in the number of epochs increase the accuracy on the test set to 98.63%. Even though is a step in the right direction is not a very significant improvement on the accuracy of the model. \n",
    "\n",
    "The log files are in thegraphcore machines with path:\n",
    "\n",
    "Graphcore/og_model.txt\n",
    "Graphcore/HW_model.txt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cerebras system\n",
    "\n",
    "Homework\n",
    "\n",
    "Run BERT example with different batch sizes like 512, 2048 and observe the performance difference.\n",
    "\n",
    "\n",
    "The initial model used a batch size of 1024, and had a performance of X. For the first example of the homework the batch size was cut to half, 512, and the performance increase/decrased as evidenced by the accuracy of ...\n",
    "\n",
    "On the other hand, when the batch size was duplicated from the original value, to have a size of 2048, the accuracy of the model ... \n",
    "\n",
    "The relevant files for this homework are available at:\n",
    "\n",
    "cerebras/batch_1024.log\n",
    "\n",
    "cerebras/batch_512.log\n",
    "\n",
    "cerebras/batch_2048.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groq \n",
    "\n",
    "Homework\n",
    "Run BERT example with custom input instead of dummy input.\n",
    "\n",
    "Followed the tutorial in the link bellow to generates tokens based on a given input using the loaded instnace of the Autotokenizer that comed from the transformers module. However, I did not succed in generating a custom dummy input. I beleive the isue is related to my text tokend not being the correct size defined by the max sequence length. Using paddind to fill the required spaces could solve the issue. \n",
    "\n",
    "The related file is at groq/ber_tiny_HW.py"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
