2024-04-09 22:48:58,786 INFO:   Effective batch size is 512.
2024-04-09 22:48:58,809 INFO:   Checkpoint autoloading is enabled. Looking for latest checkpoint in "model_dir_bert_large_pytorch" directory with the following naming convention: `checkpoint_(step)(_timestamp)?.mdl`.
2024-04-09 22:48:58,811 INFO:   No checkpoints were found in "model_dir_bert_large_pytorch".
2024-04-09 22:48:58,812 INFO:   No checkpoint was provided. Using randomly initialized model parameters.
2024-04-09 22:49:00,084 INFO:   Saving checkpoint at step 0
2024-04-09 22:49:28,614 INFO:   Saved checkpoint model_dir_bert_large_pytorch/checkpoint_0.mdl
2024-04-09 22:49:42,997 INFO:   Compiling the model. This may take a few minutes.
2024-04-09 22:49:42,999 INFO:   Defaulted to use the job-operator namespace as the usernode config /opt/cerebras/config_v2 only has access to that namespace.
2024-04-09 22:49:44,162 INFO:   Initiating a new image build job against the cluster server.
2024-04-09 22:49:44,257 INFO:   Custom worker image build is disabled from server.
2024-04-09 22:49:44,261 INFO:   Defaulted to use the job-operator namespace as the usernode config /opt/cerebras/config_v2 only has access to that namespace.
2024-04-09 22:49:44,547 INFO:   Initiating a new compile wsjob against the cluster server.
2024-04-09 22:49:44,667 INFO:   compile job id: wsjob-9qgjjtw2ttaeejtacmhk5z, remote log path: /n1/wsjob/workdir/job-operator/wsjob-9qgjjtw2ttaeejtacmhk5z
2024-04-09 22:49:54,704 INFO:   Poll ingress status: Waiting for job running, current job status: Scheduled, msg: job is scheduled. 
2024-04-09 22:50:04,703 INFO:   Poll ingress status: Waiting for job service readiness.
2024-04-09 22:50:24,757 INFO:   Ingress is ready: Job ingress ready, poll ingress success.
2024-04-09 22:50:35,921 INFO:   Pre-optimization transforms...
2024-04-09 22:50:42,371 INFO:   Optimizing layouts and memory usage...
2024-04-09 22:50:42,414 INFO:   Gradient accumulation enabled
2024-04-09 22:50:42,415 WARNING:   Gradient accumulation will search for an optimal micro batch size based on internal performance models, which can lead to an increased compile time. Specify `micro_batch_size` option in the 'train_input/eval_input' section of your .yaml parameter file to set the gradient accumulation microbatch size, if an optimal microbatch size is known.

2024-04-09 22:50:42,418 INFO:   Gradient accumulation trying sub-batch size 8...
2024-04-09 22:50:47,890 INFO:   Exploring floorplans
2024-04-09 22:50:55,516 INFO:   Exploring data layouts
2024-04-09 22:51:07,496 INFO:   Optimizing memory usage
2024-04-09 22:51:59,449 INFO:   Gradient accumulation trying sub-batch size 64...
2024-04-09 22:52:07,273 INFO:   Exploring floorplans
2024-04-09 22:52:19,244 INFO:   Exploring data layouts
2024-04-09 22:52:42,042 INFO:   Optimizing memory usage
2024-04-09 22:53:24,697 INFO:   Gradient accumulation trying sub-batch size 32...
2024-04-09 22:53:32,509 INFO:   Exploring floorplans
2024-04-09 22:53:43,183 INFO:   Exploring data layouts
2024-04-09 22:54:02,181 INFO:   Optimizing memory usage
2024-04-09 22:54:44,200 INFO:   Gradient accumulation trying sub-batch size 128...
2024-04-09 22:54:52,032 INFO:   Exploring floorplans
2024-04-09 22:55:06,192 INFO:   Exploring data layouts
2024-04-09 22:55:30,040 INFO:   Optimizing memory usage
2024-04-09 22:56:03,923 INFO:   Gradient accumulation trying sub-batch size 256...
2024-04-09 22:56:11,548 INFO:   Exploring floorplans
2024-04-09 22:56:34,690 INFO:   Exploring data layouts
2024-04-09 22:57:04,216 INFO:   Optimizing memory usage
2024-04-09 22:57:55,423 INFO:   Exploring floorplans
2024-04-09 22:58:00,639 INFO:   Exploring data layouts
2024-04-09 22:58:36,101 INFO:   Optimizing memory usage
2024-04-09 22:59:16,249 INFO:   No benefit from gradient accumulation expected. Compile will proceed at original per-box batch size 512 with 6 lanes

2024-04-09 22:59:16,298 INFO:   Post-layout optimizations...
2024-04-09 22:59:30,405 INFO:   Allocating buffers...
2024-04-09 22:59:33,812 INFO:   Code generation...
2024-04-09 22:59:49,986 INFO:   Compiling image...
2024-04-09 22:59:49,992 INFO:   Compiling kernels
2024-04-09 23:03:53,160 INFO:   Compiling final image
2024-04-09 23:07:50,141 INFO:   Compile artifacts successfully written to remote compile directory. Compile hash is: cs_8939750200954608837
2024-04-09 23:07:50,196 INFO:   Heartbeat thread stopped for wsjob-9qgjjtw2ttaeejtacmhk5z.
2024-04-09 23:07:50,199 INFO:   Compile was successful!
2024-04-09 23:07:50,204 INFO:   Programming Cerebras Wafer Scale Cluster for execution. This may take a few minutes.
2024-04-09 23:07:52,590 INFO:   Defaulted to use the job-operator namespace as the usernode config /opt/cerebras/config_v2 only has access to that namespace.
2024-04-09 23:07:52,883 INFO:   Initiating a new execute wsjob against the cluster server.
2024-04-09 23:07:53,017 INFO:   execute job id: wsjob-mgntxb429r6ewmf8xvcuur, remote log path: /n1/wsjob/workdir/job-operator/wsjob-mgntxb429r6ewmf8xvcuur
2024-04-09 23:08:03,055 INFO:   Poll ingress status: Waiting for job running, current job status: Scheduled, msg: job is scheduled. 
2024-04-09 23:08:13,096 INFO:   Poll ingress status: Waiting for job service readiness.
2024-04-09 23:08:33,225 INFO:   Poll ingress status: Waiting for job ingress readiness.
2024-04-09 23:08:53,272 INFO:   Ingress is ready: Job ingress ready, poll ingress success.
2024-04-09 23:09:23,407 INFO:   Preparing to execute using 1 CSX
2024-04-09 23:09:52,957 INFO:   About to send initial weights
2024-04-09 23:10:41,045 INFO:   Finished sending initial weights
2024-04-09 23:10:41,048 INFO:   Finalizing appliance staging for the run
2024-04-09 23:10:41,085 INFO:   Waiting for device programming to complete
2024-04-09 23:12:48,619 INFO:   Device programming is complete
2024-04-09 23:12:49,489 INFO:   Using network type: ROCE
2024-04-09 23:12:49,490 INFO:   Waiting for input workers to prime the data pipeline and begin streaming ...
2024-04-09 23:12:49,518 INFO:   Input workers have begun streaming input data
2024-04-09 23:13:06,347 INFO:   Appliance staging is complete
2024-04-09 23:13:06,352 INFO:   Beginning appliance run
2024-04-09 23:13:23,668 INFO:   | Train Device=CSX, Step=100, Loss=9.39062, Rate=2968.13 samples/sec, GlobalRate=2968.13 samples/sec
2024-04-09 23:13:41,113 INFO:   | Train Device=CSX, Step=200, Loss=8.70312, Rate=2948.29 samples/sec, GlobalRate=2951.51 samples/sec
2024-04-09 23:13:58,915 INFO:   | Train Device=CSX, Step=300, Loss=7.79688, Rate=2904.94 samples/sec, GlobalRate=2925.92 samples/sec
2024-04-09 23:14:16,394 INFO:   | Train Device=CSX, Step=400, Loss=7.39062, Rate=2919.50 samples/sec, GlobalRate=2926.74 samples/sec
2024-04-09 23:14:33,958 INFO:   | Train Device=CSX, Step=500, Loss=7.80469, Rate=2916.82 samples/sec, GlobalRate=2924.39 samples/sec
2024-04-09 23:14:51,514 INFO:   | Train Device=CSX, Step=600, Loss=7.53125, Rate=2916.59 samples/sec, GlobalRate=2923.06 samples/sec
2024-04-09 23:15:09,046 INFO:   | Train Device=CSX, Step=700, Loss=7.35156, Rate=2918.80 samples/sec, GlobalRate=2922.66 samples/sec
2024-04-09 23:15:26,542 INFO:   | Train Device=CSX, Step=800, Loss=7.27344, Rate=2923.37 samples/sec, GlobalRate=2923.13 samples/sec
2024-04-09 23:15:44,154 INFO:   | Train Device=CSX, Step=900, Loss=7.35938, Rate=2913.63 samples/sec, GlobalRate=2921.35 samples/sec
2024-04-09 23:16:01,709 INFO:   | Train Device=CSX, Step=1000, Loss=7.12500, Rate=2915.42 samples/sec, GlobalRate=2920.87 samples/sec
2024-04-09 23:16:01,709 INFO:   Saving checkpoint at step 1000
2024-04-09 23:17:06,127 INFO:   Saved checkpoint model_dir_bert_large_pytorch/checkpoint_1000.mdl
2024-04-09 23:17:44,442 INFO:   Heartbeat thread stopped for wsjob-mgntxb429r6ewmf8xvcuur.
2024-04-09 23:17:44,450 INFO:   Training completed successfully!
2024-04-09 23:17:44,450 INFO:   Processed 512000 sample(s) in 175.290186704 seconds.
